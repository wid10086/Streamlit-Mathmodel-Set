import os
os.environ['OMP_NUM_THREADS'] = '1'
import streamlit as st
import pandas as pd
import numpy as np
from io import BytesIO
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import cohen_kappa_score
from scipy.cluster.hierarchy import linkage
from scipy.stats import pearsonr, spearmanr
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import seaborn as sns
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['SimHei']  # æŒ‡å®šé»˜è®¤å­—ä½“
plt.rcParams['font.family']='sans-serif'  # è®¾ç½®å­—ä½“æ ·å¼
plt.rcParams['axes.unicode_minus'] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·
# ================= ç›®æ ‡ä»£ç ä¸­çš„å‡½æ•°ï¼ˆä¿æŒåŸæ ·ï¼‰ =================

def pca_reduction(data, n_components=None):
    """
    ä½¿ç”¨PCAæ–¹æ³•è¿›è¡Œé™ç»´
    :param data: numpyæ•°ç»„ï¼Œshapeä¸º(n_samples, n_features_X)çš„è¾“å…¥æ•°æ®
    :param n_components: intï¼Œè¦é™è‡³çš„ç»´æ•°ï¼Œé»˜è®¤ä¸ºNone(è‡ªåŠ¨é€‰æ‹©ä¿ç•™95%æ–¹å·®çš„ç»´æ•°)
    :return: numpyæ•°ç»„ï¼Œé™ç»´åçš„æ•°æ®
    """
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    pca = PCA(n_components=n_components)
    return pca.fit_transform(data_scaled)


def fisher_lda(data, labels, n_components=2):
    """
    ä½¿ç”¨Fisherçº¿æ€§åˆ¤åˆ«åˆ†æè¿›è¡Œé™ç»´
    :param data: numpyæ•°ç»„ï¼Œshapeä¸º(n_samples, n_features_X)çš„è¾“å…¥æ•°æ®
    :param labels: numpyæ•°ç»„ï¼Œæ•°æ®æ ‡ç­¾æ®æ ‡ç­¾
    :param n_components: intï¼Œè¦é™è‡³çš„ç»´æ•°
    :return: numpyæ•°ç»„ï¼Œé™ç»´åçš„æ•°æ®
    """
    classes = np.unique(labels)
    total_mean = np.mean(data, axis=0)

    # è®¡ç®—ç±»å†…æ•£åº¦çŸ©é˜µå’Œç±»é—´æ•£åº¦çŸ©é˜µ
    Sw = np.zeros((data.shape[1], data.shape[1]))
    Sb = np.zeros((data.shape[1], data.shape[1]))

    for c in classes:
        class_data = data[labels == c]
        if len(class_data) < 1:
            continue  # è·³è¿‡ç©ºç±»åˆ«
        class_mean = np.mean(class_data, axis=0)
        Sw += np.dot((class_data - class_mean).T, (class_data - class_mean))
        diff = (class_mean - total_mean).reshape(-1, 1)
        Sb += len(class_data) * np.dot(diff, diff.T)

    # æ·»åŠ æ­£åˆ™åŒ–é¡¹
    Sw += 1e-6 * np.eye(Sw.shape[0])

    # ä½¿ç”¨ä¼ªé€†å¹¶æå–å®éƒ¨
    eigvals, eigvecs = np.linalg.eig(np.linalg.pinv(Sw).dot(Sb))
    eigvals = np.real(eigvals)
    eigvecs = np.real(eigvecs)

    eiglist = sorted(zip(eigvals, eigvecs.T), key=lambda x: x[0], reverse=True)
    W = np.hstack([eigvec.reshape(-1, 1) for eigval, eigvec in eiglist[:n_components]])
    return np.dot(data, W)


def pearson_correlation(x, y):
    """
    è®¡ç®—ä¸¤ä¸ªå˜é‡çš„çš®å°”é€Šç›¸å…³ç³»æ•°
    :param x: numpyæ•°ç»„æˆ–åˆ—è¡¨ï¼Œç¬¬ä¸€ä¸ªå˜é‡
    :param y: numpyæ•°ç»„æˆ–åˆ—è¡¨ï¼Œç¬¬äºŒä¸ªå˜é‡
    :return: floatï¼Œç›¸å…³ç³»æ•°å’Œpå€¼
    """
    return pearsonr(x, y)


def spearman_correlation(x, y):
    """
    è®¡ç®—ä¸¤ä¸ªå˜é‡çš„æ–¯çš®å°”æ›¼ç§©ç›¸å…³ç³»æ•°
    :param x: numpyæ•°ç»„æˆ–åˆ—è¡¨ï¼Œç¬¬ä¸€ä¸ªå˜é‡
    :param y: numpyæ•°ç»„æˆ–åˆ—è¡¨ï¼Œç¬¬äºŒä¸ªå˜é‡
    :return: floatï¼Œç›¸å…³ç³»æ•°å’Œpå€¼
    """
    return spearmanr(x, y)


def kappa_coefficient(rater1, rater2):
    """
    è®¡ç®—ä¸¤ä¸ªè¯„åˆ†è€…ä¹‹é—´çš„Kappaä¸€è‡´æ€§ç³»æ•°
    :param rater1: numpyæ•°ç»„ï¼Œç¬¬ä¸€ä¸ªè¯„åˆ†è€…çš„è¯„åˆ†
    :param rater2: numpyæ•°ç»„ï¼Œç¬¬äºŒä¸ªè¯„åˆ†è€…çš„è¯„åˆ†
    :return: floatï¼ŒKappaç³»æ•°
    """
    return cohen_kappa_score(rater1, rater2)


def kmeans_clustering(data, n_clusters=3, random_state=42):
    """
    ä½¿ç”¨K-meansç®—æ³•è¿›è¡Œèšç±»
    :param data: numpyæ•°ç»„ï¼Œshapeä¸º(n_samples, n_features_X)çš„è¾“å…¥æ•°æ®
    :param n_clusters: intï¼Œèšç±»çš„æ•°é‡
    :param random_state: intï¼Œéšæœºç§å­
    :return: numpyæ•°ç»„ï¼Œèšç±»æ ‡ç­¾
    """
    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)
    return kmeans.fit_predict(data)


def dbscan_clustering(data, eps=0.5, min_samples=5):
    """
    ä½¿ç”¨DBSCANç®—æ³•è¿›è¡Œèšç±»
    :param data: numpyæ•°ç»„ï¼Œshapeä¸º(n_samples, n_features_X)çš„è¾“å…¥æ•°æ®
    :param eps: floatï¼Œé‚»åŸŸåŠå¾„
    :param min_samples: intï¼Œæˆä¸ºæ ¸å¿ƒç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°
    :return: numpyæ•°ç»„ï¼Œèšç±»æ ‡ç­¾
    """
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    return dbscan.fit_predict(data)


def hierarchical_clustering(data, method='ward'):
    """
    ä½¿ç”¨å±‚æ¬¡èšç±»ç®—æ³•è¿›è¡Œèšç±»
    :param data: numpyæ•°ç»„ï¼Œshapeä¸º(n_samples, n_features_X)çš„è¾“å…¥æ•°æ®
    :param method: strï¼Œèšç±»æ–¹æ³•ï¼Œå¯é€‰'ward'ï¼Œ'complete'ï¼Œ'average'ï¼Œ'single'
    :return: numpyæ•°ç»„ï¼Œå±‚æ¬¡èšç±»çš„è¿æ¥çŸ©é˜µ
    """
    return linkage(data, method=method)


class Autoencoder(nn.Module):
    def __init__(self, input_dim, encoding_dim):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, encoding_dim * 2),
            nn.ReLU(),
            nn.Linear(encoding_dim * 2, encoding_dim),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim, encoding_dim * 2),
            nn.ReLU(),
            nn.Linear(encoding_dim * 2, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

    def encode(self, x):
        return self.encoder(x)

@st.fragment()
def autoencoder_reduction(data, encoding_dim=2, epochs=50, batch_size=32, learning_rate=1e-3):
    """
    ä½¿ç”¨PyTorchå®ç°çš„è‡ªç¼–ç å™¨è¿›è¡Œé™ç»´
    :param data: numpyæ•°ç»„ï¼Œshapeä¸º(n_samples, n_features_X)çš„è¾“å…¥æ•°æ®
    :param encoding_dim: intï¼Œè¦é™è‡³çš„ç»´æ•°
    :param epochs: intï¼Œè®­ç»ƒè½®æ•°
    :param batch_size: intï¼Œæ‰¹æ¬¡å¤§å°
    :param learning_rate: floatï¼Œå­¦ä¹ ç‡
    :return: numpyæ•°ç»„ï¼Œé™ç»´åçš„æ•°æ®
    """
    # è½¬æ¢æ•°æ®ä¸ºtorchå¼ é‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    data_tensor = torch.FloatTensor(data).to(device)
    input_dim = data.shape[1]

    # åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    model = Autoencoder(input_dim, encoding_dim).to(device)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = TensorDataset(data_tensor, data_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # è®­ç»ƒæ¨¡å‹
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_features, _ in dataloader:
            optimizer.zero_grad()
            outputs = model(batch_features)
            loss = criterion(outputs, batch_features)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        if (epoch + 1) % 10 == 0:
            avg_loss = total_loss / len(dataloader)
            print(f'Epoch [{epoch + 1}/{epochs}], Average Loss: {avg_loss:.4f}')

    # è·å–ç¼–ç åçš„æ•°æ®
    model.eval()
    with torch.no_grad():
        encoded_data = model.encode(data_tensor)

    return encoded_data.cpu().numpy()


# ================= é¡µé¢é…ç½® =================
st.set_page_config(layout="wide", page_title="ğŸ” æ•°æ®æŒ–æ˜åˆ†æ")
st.title("ğŸ” æ•°æ®æŒ–æ˜åˆ†æ")

# ================= æ•°æ®é…ç½® =================
with st.expander("ğŸ“Š æ•°æ®é…ç½®", expanded=True):
    data_source = st.radio(
        "è¯·é€‰æ‹©æ•°æ®æ¥æº",
        options=["ç”Ÿæˆç¤ºä¾‹æ•°æ®", "ä¸Šä¼ è‡ªå®šä¹‰æ•°æ®"],
        horizontal=True,
        key="data_mining_data_source_radio"
    )

    if data_source == "ç”Ÿæˆç¤ºä¾‹æ•°æ®":
        st.subheader("âœ¨ ç¤ºä¾‹æ•°æ®ç”Ÿæˆ")
        x1, x2 = st.columns(2)
        method_type = st.session_state.get("data_mining_method", "é™ç»´åˆ†æ")
        n_features = x1.number_input("ç‰¹å¾æ•°é‡", 2, 50, 10, key="n_features")
        n_samples = x2.number_input("æ ·æœ¬æ•°é‡", 50, 1000, 200, key="n_samples")
        add_noise = x1.checkbox("æ·»åŠ å™ªå£°", key="add_noise")
        add_outliers = x2.checkbox("æ·»åŠ ç¦»ç¾¤ç‚¹", key="add_outliers")

        with st.form("data_mining_example_form"):
            submit_example = st.form_submit_button("ç”Ÿæˆç¤ºä¾‹æ•°æ®")
            if submit_example:
                np.random.seed(42)
                X = np.random.randn(n_samples, n_features)
                if add_noise:
                    X += np.random.normal(0, 0.5, X.shape)
                if add_outliers:
                    outlier_idx = np.random.choice(n_samples, size=max(3, n_samples // 20), replace=False)
                    X[outlier_idx] *= 5
                y = np.random.randint(0, 3, n_samples)
                df = pd.DataFrame(X, columns=[f"Feature_{i}" for i in range(n_features)])
                df["Label"] = y
                st.session_state.data_mining_data = df
                st.success("ç¤ºä¾‹æ•°æ®ç”ŸæˆæˆåŠŸï¼")

    elif data_source == "ä¸Šä¼ è‡ªå®šä¹‰æ•°æ®":
        st.subheader("ğŸ“¤ ä¸Šä¼ è‡ªå®šä¹‰æ•°æ®")
        with st.form("data_mining_upload_form"):
            uploaded_file = st.file_uploader("ä¸Šä¼ CSV/Excelæ–‡ä»¶", type=["csv", "xlsx"], key="data_mining_upload")
            submit_upload = st.form_submit_button("ä¸Šä¼ æ•°æ®")
            if submit_upload and uploaded_file:
                try:
                    if uploaded_file.name.endswith('.csv'):
                        df = pd.read_csv(uploaded_file)
                    else:
                        df = pd.read_excel(uploaded_file)
                    label_col = st.selectbox("é€‰æ‹©æ ‡ç­¾åˆ—ï¼ˆå¯é€‰ï¼‰", ["æ— "] + list(df.columns), key="data_mining_label_col")
                    if label_col != "æ— ":
                        df = df.rename(columns={label_col: "Label"})
                    st.session_state.data_mining_data = df
                    st.success("æ•°æ®åŠ è½½æˆåŠŸï¼")
                except Exception as e:
                    st.error(f"æ•°æ®åŠ è½½é”™è¯¯: {str(e)}")
            elif submit_upload and not uploaded_file:
                st.warning("è¯·å…ˆä¸Šä¼ æ–‡ä»¶ã€‚")

# ================= æ•°æ®é¢„è§ˆ =================
with st.expander("ğŸ” æ•°æ®é¢„è§ˆ", expanded=False):
    if "data_mining_data" not in st.session_state:
        st.warning("è¯·å…ˆç”Ÿæˆæˆ–ä¸Šä¼ æ•°æ®")
        st.stop()
    df = st.session_state.data_mining_data
    cols = st.columns([2, 1])
    cols[0].dataframe(df)
    cols[1].write(f"ğŸ“ æ•°æ®ç»´åº¦ï¼š{df.shape}")

# ================= æ–¹æ³•é€‰æ‹©ä¸å‚æ•°é…ç½® =================
st.markdown("---")
method_options = {
    "é™ç»´åˆ†æ": ["PCA", "Fisher LDA", "è‡ªç¼–ç å™¨"],
    "ç›¸å…³æ€§åˆ†æ": ["çš®å°”é€Šç›¸å…³ç³»æ•°", "æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°"],
    "ä¸€è‡´æ€§åˆ†æ": ["Kappaç³»æ•°"],
    "èšç±»åˆ†æ": ["K-means", "DBSCAN", "å±‚æ¬¡èšç±»"]
}

method_category = st.selectbox("ğŸ§® é€‰æ‹©æ–¹æ³•ç±»åˆ«", list(method_options.keys()))
method = st.selectbox("ğŸ§© é€‰æ‹©å…·ä½“æ–¹æ³•", method_options[method_category])

params = {}
selected_features = []  # æ–°å¢ï¼šå­˜å‚¨é€‰æ‹©çš„ç‰¹å¾
with st.expander("âš™ï¸ å‚æ•°é…ç½®"):
    if method == "PCA":
        params["n_components"] = st.number_input("é™ç»´ç»´åº¦", 1, 10, 2)
    elif method == "Fisher LDA":
        params["n_components"] = st.number_input("é™ç»´ç»´åº¦", 1, df.shape[1]-1, 2)
    elif method == "è‡ªç¼–ç å™¨":
        params["encoding_dim"] = st.number_input("ç¼–ç ç»´åº¦", 1, 10, 2)
    elif method == "Kappaç³»æ•°":
        col1, col2 = st.columns(2)
        rater1 = col1.selectbox("é€‰æ‹©è¯„åˆ†è€…1", df.columns)
        rater2 = col2.selectbox("é€‰æ‹©è¯„åˆ†è€…2", df.columns)

        # æ–°å¢ï¼šè¿ç»­æ•°æ®ç¦»æ•£åŒ–å¤„ç†
        if np.issubdtype(df[rater1].dtype, np.number) or np.issubdtype(df[rater2].dtype, np.number):
            st.warning("æ£€æµ‹åˆ°è¿ç»­å‹æ•°æ®ï¼ŒKappaåˆ†æéœ€è¦ç¦»æ•£å€¼ï¼Œè¯·è®¾ç½®åˆ†ç®±å‚æ•°")

            bin_method = st.selectbox("åˆ†ç®±æ–¹æ³•", ["ç­‰å®½åˆ†ç®±", "ç­‰é¢‘åˆ†ç®±"])
            n_bins = st.number_input("åˆ†ç®±æ•°é‡", 2, 10, 3)
            params["bin_method"] = bin_method
            params["n_bins"] = n_bins

            if bin_method == "ç­‰å®½åˆ†ç®±":
                rater1_binned = pd.cut(df[rater1], bins=n_bins, labels=False)
                rater2_binned = pd.cut(df[rater2], bins=n_bins, labels=False)
            else:
                rater1_binned = pd.qcut(df[rater1], q=n_bins, labels=False, duplicates='drop')
                rater2_binned = pd.qcut(df[rater2], q=n_bins, labels=False, duplicates='drop')

            params["rater1"] = rater1_binned
            params["rater2"] = rater2_binned

        else:
            params["rater1"] = df[rater1]
            params["rater2"] = df[rater2]

    elif method in ["K-means"]:
        params["n_clusters"] = st.number_input("èšç±»æ•°é‡", 2, 10, 3)
    elif method in [ "å±‚æ¬¡èšç±»"]:
        params["n_clusters"] = st.number_input("èšç±»æ•°é‡", 2, 10, 3)
        params["ccway"] = st.selectbox("é€‰æ‹©æ–¹æ³•",['ward','complete','average','single'])
    elif method == "DBSCAN":
        params["eps"] = st.number_input("é‚»åŸŸåŠå¾„", 0.1, 5.0, 0.5)
        params["min_samples"] = st.number_input("æœ€å°æ ·æœ¬æ•°", 1, 20, 5)
    # æ–°å¢ï¼šç›¸å…³æ€§åˆ†æç‰¹å¾é€‰æ‹©
    elif method in ["çš®å°”é€Šç›¸å…³ç³»æ•°", "æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°"]:
        available_features = df.columns.tolist()
        if "Label" in available_features:
            available_features.remove("Label")
        selected_features = st.multiselect(
            "é€‰æ‹©åˆ†æç‰¹å¾ï¼ˆå¯å¤šé€‰ï¼‰",
            available_features,
            default=available_features[:2] if len(available_features)>=2 else []
        )
# ================= æ‰§è¡Œåˆ†æ =================
if st.button("ğŸš€ å¼€å§‹åˆ†æ") and "data_mining_data" in st.session_state:
    df = st.session_state.data_mining_data
    results = {}

    try:
        if method == "PCA":
            X = df.drop(columns=["Label"]) if "Label" in df.columns else df
            reduced_data = pca_reduction(X.values, **params)
            results["result"] = pd.DataFrame(reduced_data, columns=[f"PC_{i}" for i in range(reduced_data.shape[1])])

        elif method == "Fisher LDA":
            X = df.drop(columns=["Label"]).values
            y = df["Label"].values
            reduced_data = fisher_lda(X, y, **params)
            results["result"] = pd.DataFrame(reduced_data, columns=[f"LD_{i}" for i in range(reduced_data.shape[1])])

        elif method == "è‡ªç¼–ç å™¨":
            X = StandardScaler().fit_transform(df.drop(columns=["Label"]) if "Label" in df.columns else df)
            reduced_data = autoencoder_reduction(X, **params)
            results["result"] = pd.DataFrame(reduced_data, columns=[f"AE_{i}" for i in range(reduced_data.shape[1])])


        elif method in ["çš®å°”é€Šç›¸å…³ç³»æ•°", "æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°"]:
            corr_func = pearson_correlation if method == "çš®å°”é€Šç›¸å…³ç³»æ•°" else spearman_correlation
            # æ–°å¢ï¼šå¤šç‰¹å¾å¤„ç†
            if len(selected_features) < 2:
                st.error("è¯·è‡³å°‘é€‰æ‹©ä¸¤ä¸ªç‰¹å¾è¿›è¡Œåˆ†æ")
                st.stop()
            # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
            corr_matrix = np.zeros((len(selected_features), len(selected_features)))
            p_matrix = np.zeros((len(selected_features), len(selected_features)))
            for i, feat1 in enumerate(selected_features):
                for j, feat2 in enumerate(selected_features):
                    corr, p = corr_func(df[feat1], df[feat2])
                    corr_matrix[i][j] = corr
                    p_matrix[i][j] = p

            # åˆ›å»ºå®Œæ•´çš„ç›¸å…³ç³»æ•°çŸ©é˜µDataFrame
            corr_df = pd.DataFrame(
                corr_matrix,
                index=selected_features,
                columns=selected_features
            )
            results["result"] = corr_df  # ç¡®ä¿resultå­—æ®µå§‹ç»ˆæœ‰å€¼
            results["corr_matrix"] = corr_matrix
            results["p_matrix"] = p_matrix
            results["selected_features"] = selected_features
            # ä¿ç•™åŸæœ‰åŒç‰¹å¾æ˜¾ç¤º
            if len(selected_features) == 2:
                results["result"] = pd.DataFrame([[corr_matrix[0][1], p_matrix[0][1]]],columns=["ç›¸å…³ç³»æ•°", "på€¼"])

        elif method == "Kappaç³»æ•°":
            if np.issubdtype(df[rater1].dtype, np.number) or np.issubdtype(df[rater2].dtype, np.number):
                kappa = kappa_coefficient(params["rater1"], params["rater2"])
                results["result"] = pd.DataFrame([[kappa, params["bin_method"], params["n_bins"]]],
                                                 columns=["Kappaç³»æ•°", "åˆ†ç®±æ–¹æ³•", "åˆ†ç®±æ•°é‡"])
            else:
                kappa = kappa_coefficient(params["rater1"], params["rater2"])
                results["result"] = pd.DataFrame([[kappa]], columns=["Kappaç³»æ•°"])

        elif method == "K-means":
            X = StandardScaler().fit_transform(df)
            labels = kmeans_clustering(X, **params)
            results["result"] = pd.DataFrame(labels, columns=["Cluster"])

        elif method == "DBSCAN":
            X = StandardScaler().fit_transform(df)
            labels = dbscan_clustering(X, **params)
            results["result"] = pd.DataFrame(labels, columns=["Cluster"])

        elif method == "å±‚æ¬¡èšç±»":
            X = StandardScaler().fit_transform(df)
            linkage_matrix = hierarchical_clustering(X,params["ccway"])
            results["result"] = pd.DataFrame(linkage_matrix)

        st.session_state.data_mining_results = results
        st.success("åˆ†æå®Œæˆï¼")

    except Exception as e:
        st.error(f"åˆ†æå¤±è´¥: {str(e)}")

# ================= ç»“æœå±•ç¤º =================
if "data_mining_results" in st.session_state:
    st.markdown("---")
    with st.expander("ğŸ“ˆ åˆ†æç»“æœ", expanded=True):
        result_df = st.session_state.data_mining_results.get("result")
        corr_matrix = st.session_state.data_mining_results.get("corr_matrix")
        selected_features = st.session_state.data_mining_results.get("selected_features")
        linkage_matrix = st.session_state.data_mining_results.get("linkage_matrix")

        # å¯è§†åŒ–å±•ç¤ºé‡æ„
        col1, col2 = st.columns([2, 1])
        with col1:
            if method in ["PCA", "Fisher LDA", "è‡ªç¼–ç å™¨"]:
                st.subheader("é™ç»´å¯è§†åŒ–(ä»…å‰ä¸¤ä¸ªç»´åº¦)")
                if result_df.shape[1] >= 2:
                    plt.figure(figsize=(8, 6))
                    sns.scatterplot(
                        x=result_df.iloc[:, 0],
                        y=result_df.iloc[:, 1],
                        hue=df["Label"] if "Label" in df.columns else None,
                        palette="viridis"
                    )
                    plt.xlabel(result_df.columns[0])
                    plt.ylabel(result_df.columns[1])
                    plt.title(f"{method} é™ç»´åˆ†å¸ƒ")
                    st.pyplot(plt)
                else:
                    st.line_chart(result_df)

            elif method in ["çš®å°”é€Šç›¸å…³ç³»æ•°", "æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°"]:
                if len(selected_features) == 2:
                    st.subheader("ç‰¹å¾æ•£ç‚¹å›¾")
                    sns.jointplot(
                        data=df,
                        x=selected_features[0],
                        y=selected_features[1],
                        kind="reg"
                    )
                    st.pyplot(plt)
                else:
                    st.subheader("ç›¸å…³ç³»æ•°çŸ©é˜µ")
                    plt.figure(figsize=(10, 8))
                    sns.heatmap(
                        corr_matrix,
                        annot=True,
                        fmt=".2f",
                        cmap="coolwarm",
                        xticklabels=selected_features,
                        yticklabels=selected_features
                    )
                    st.pyplot(plt)

            elif method in ["K-means", "DBSCAN"]:
                st.subheader("èšç±»åˆ†å¸ƒ")
                if result_df.shape[0] == df.shape[0]:
                    plot_df = pd.concat([
                        df.drop(columns=["Label"], errors="ignore"),
                        result_df
                    ], axis=1)
                    plt.figure(figsize=(8, 6))
                    sns.scatterplot(
                        data=plot_df,
                        x=plot_df.columns[0],
                        y=plot_df.columns[1],
                        hue="Cluster",
                        palette="tab10"
                    )
                    st.pyplot(plt)

            elif method == "å±‚æ¬¡èšç±»":
                st.subheader("æ ‘çŠ¶å›¾")
                plt.figure(figsize=(12, 6))
                dendrogram = sns.clustermap(
                    df.drop(columns=["Label"], errors="ignore"),
                    row_linkage=linkage_matrix,
                    col_cluster=False
                )
                st.pyplot(dendrogram.fig)

        # ä¸‹è½½åŠŸèƒ½å¢å¼º
        with col2:
            excel_buffer = BytesIO()
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                # é€šç”¨ç»“æœä¿å­˜
                result_df.to_excel(writer, sheet_name='åˆ†æç»“æœ')

                # æ–¹æ³•ç‰¹å®šæ•°æ®ä¿å­˜
                if method in ["çš®å°”é€Šç›¸å…³ç³»æ•°", "æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°"]:
                    pd.DataFrame(
                        corr_matrix,
                        columns=selected_features,
                        index=selected_features
                    ).to_excel(writer, sheet_name='ç›¸å…³ç³»æ•°çŸ©é˜µ')

                    pd.DataFrame(
                        st.session_state.data_mining_results.get("p_matrix"),
                        columns=selected_features,
                        index=selected_features
                    ).to_excel(writer, sheet_name='På€¼çŸ©é˜µ')

                if method == "å±‚æ¬¡èšç±»":
                    pd.DataFrame(
                        linkage_matrix,
                        columns=['Cluster1', 'Cluster2', 'Distance', 'SampleCount']
                    ).to_excel(writer, sheet_name='è¿æ¥çŸ©é˜µ')

                if method in ["K-means", "DBSCAN"]:
                    cluster_profile = df.groupby(result_df['Cluster']).mean()
                    cluster_profile.to_excel(writer, sheet_name='èšç±»ä¸­å¿ƒç‰¹å¾')

            # æ˜¾ç¤ºç²¾ç®€ç»“æœ
            st.subheader("â­ æ ¸å¿ƒæŒ‡æ ‡")
            if method == "Kappaç³»æ•°":
                st.metric(label="Kappaç³»æ•°", value=f"{result_df.iloc[0, 0]:.3f}")
            elif method in ["çš®å°”é€Šç›¸å…³ç³»æ•°", "æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°"] and len(selected_features) == 2:
                st.metric(label="ç›¸å…³ç³»æ•°", value=f"{result_df.iloc[0, 0]:.3f}")
                st.metric(label="På€¼", value=f"{result_df.iloc[0, 1]:.4f}")
            else:
                st.dataframe(result_df)
            st.download_button(
                label="â¬‡ï¸ ä¸‹è½½å®Œæ•´åˆ†æç»“æœ",
                data=excel_buffer.getvalue(),
                file_name=f"{method}_åˆ†æç»“æœ.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )